{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f50315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from IPython.core.display import HTML\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.sparse.linalg import svds\n",
    "from sklearn.metrics import mean_squared_error, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "416f614e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import Dataset, Reader\n",
    "from surprise.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba28c6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_books = pd.read_csv(\"cleaned_books.csv\")\n",
    "df_ratings = pd.read_csv(\"cleaned_ratings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a90176f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# limit the ratings per book to max 1000\n",
    "df_ratings = df_ratings.groupby('user_id').filter(lambda x: len(x) >= 5)\n",
    "df_ratings = df_ratings[df_ratings[\"book_id\"].isin(df_books.index)]\n",
    "df_ratings = (\n",
    "    df_ratings.groupby(\"book_id\")\n",
    "    .apply(lambda x: x.sample(min(1000, len(x))))\n",
    "    .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b9a03878",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3457285"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_ratings)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dab4437e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map IDs to indices\n",
    "user_map = {u: i for i, u in enumerate(df_ratings['user_id'].unique())}\n",
    "item_map = {b: i for i, b in enumerate(df_ratings['book_id'].unique())}\n",
    "df_ratings['user_idx'] = df_ratings['user_id'].map(user_map)\n",
    "df_ratings['item_idx'] = df_ratings['book_id'].map(item_map)\n",
    "\n",
    "n_users = len(user_map)\n",
    "n_items = len(item_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "81afc57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df_ratings, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5e8c69fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "R_train = np.zeros((n_users, n_items))\n",
    "\n",
    "for row in train_df.itertuples():\n",
    "    R_train[row.user_idx, row.item_idx] = row.rating\n",
    "    \n",
    "# SCompute user mean ratings (avoid divide by zero)\n",
    "user_means = np.true_divide(R_train.sum(1), (R_train != 0).sum(1))\n",
    "user_means = np.nan_to_num(user_means)  # convert NaNs to 0 if needed\n",
    "\n",
    "# Center the ratings (subtract user mean from each rating)\n",
    "R_centered = R_train - user_means[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3bfd1d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of latent factors\n",
    "k = 20  \n",
    "U, sigma, Vt = svds(R_centered, k=k)\n",
    "sigma = np.diag(sigma)\n",
    "\n",
    "predicted_ratings_matrix = np.dot(np.dot(U, sigma), Vt)\n",
    "predicted_ratings_matrix += user_means[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9105f9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rmse(pred_matrix, test_df):\n",
    "    preds = []\n",
    "    truths = []\n",
    "\n",
    "    for row in test_df.itertuples():\n",
    "        user_idx = row.user_idx\n",
    "        item_idx = row.item_idx\n",
    "        pred_rating = pred_matrix[user_idx, item_idx]\n",
    "        preds.append(pred_rating)\n",
    "        truths.append(row.rating)\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(truths, preds))\n",
    "    return round(rmse, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3df5ee4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mae(pred_matrix, test_df):\n",
    "    preds = []\n",
    "    truths = []\n",
    "\n",
    "    for row in test_df.itertuples():\n",
    "        user_idx = row.user_idx\n",
    "        item_idx = row.item_idx\n",
    "        pred_rating = pred_matrix[user_idx, item_idx]\n",
    "        preds.append(pred_rating)\n",
    "        truths.append(row.rating)\n",
    "\n",
    "    mae = mean_absolute_error(truths, preds)\n",
    "    return round(mae, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7963458",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apk(actual, predicted, k=5):\n",
    "    if not actual:\n",
    "        return 0.0\n",
    "    predicted = predicted[:k]\n",
    "\n",
    "    score = 0.0\n",
    "    hits = 0\n",
    "    for i, p in enumerate(predicted):\n",
    "        if p in actual and p not in predicted[:i]:\n",
    "            hits += 1\n",
    "            score += hits / (i + 1)\n",
    "    return score / min(len(actual), k)\n",
    "\n",
    "def reciprocal_rank(actual, predicted):\n",
    "    for i, p in enumerate(predicted):\n",
    "        if p in actual:\n",
    "            return 1.0 / (i + 1)\n",
    "    return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ff8f068",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_recall_f1_map_mrr(pred_matrix, train_df, test_df, k=10, threshold=4.0):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_score = 0\n",
    "    total_rr = 0\n",
    "    total_relevant = 0\n",
    "    user_count = 0\n",
    "    train_user_items = train_df.groupby(\"user_idx\")[\"item_idx\"].apply(set)\n",
    "\n",
    "    for user in tqdm(test_df['user_idx'].unique(), desc=\"Processing evaluation (Precision, Recall, F1, MRR, MAP)\", unit=\"user\"):\n",
    "        rated_items = train_user_items.get(user, set())\n",
    "        user_pred = pred_matrix[user]\n",
    "        \n",
    "        # Exclude items seen in training\n",
    "        unseen_items = np.setdiff1d(np.arange(pred_matrix.shape[1]), list(rated_items))\n",
    "        top_k_items = unseen_items[np.argsort(user_pred[unseen_items])[::-1][:k]]\n",
    "\n",
    "        # True relevant items in test set for this user\n",
    "        user_test = test_df[test_df['user_idx'] == user]\n",
    "        relevant_items = user_test[user_test['rating'] >= threshold]['item_idx'].values\n",
    "        \n",
    "        # Calculate precision\n",
    "        correct += len(set(top_k_items) & set(relevant_items))\n",
    "        total += k\n",
    "        total_relevant += len(relevant_items)\n",
    "        \n",
    "        actual_items = test_df[(test_df['user_idx'] == user) & (test_df['rating'] >= threshold)]['item_idx'].tolist()\n",
    "        #Calculate MAP and MRR\n",
    "        if actual_items:\n",
    "            total_score += apk(actual_items, list(top_k_items), k)\n",
    "            total_rr += reciprocal_rank(actual_items, list(top_k_items))\n",
    "            user_count += 1      \n",
    "\n",
    "    precision = round(correct / total, 4)\n",
    "    recall = round(correct / total_relevant, 4) if total_relevant else 0\n",
    "    f1 = round(2 * precision * recall / (precision + recall), 4) if (precision + recall) > 0 else 0\n",
    "    map = round(total_score / user_count, 4) if user_count else 0\n",
    "    mrr = round(total_rr / user_count, 4) if user_count else 0\n",
    "    return precision, recall, f1, map, mrr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "79e7dae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 3.9117\n",
      "MAE: 3.7898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing evaluation (Precision, Recall, F1, MRR, MAP): 100%|██████████| 53402/53402 [11:14<00:00, 79.15user/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision@5: 0.0296\n",
      "Recall@5: 0.0167\n",
      "F1@5: 0.0214\n",
      "MAP@5: 0.0198\n",
      "MRR@5: 0.0601\n"
     ]
    }
   ],
   "source": [
    "print(\"RMSE:\", compute_rmse(predicted_ratings_matrix, test_df))\n",
    "print(\"MAE:\", compute_mae(predicted_ratings_matrix, test_df))\n",
    "\n",
    "precision, recall, f1, map, mrr = pre_recall_f1_map_mrr(predicted_ratings_matrix, train_df, test_df, k=5, threshold=4.0)\n",
    "print(\"Precision@5:\", precision)\n",
    "print(\"Recall@5:\", recall)\n",
    "print(\"F1@5:\", f1)\n",
    "print(\"MAP@5:\", map)\n",
    "print(\"MRR@5:\", mrr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4f2d49a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_books(user_id, R_train, predicted_ratings, n=5):\n",
    "    user_idx = user_map[user_id]\n",
    "    user_ratings = R_train[user_idx]\n",
    "    preds = predicted_ratings[user_idx]\n",
    "    \n",
    "    # Books not rated by user\n",
    "    unrated_indices = np.where(user_ratings == 0)[0]\n",
    "    recommended_indices = unrated_indices[np.argsort(preds[unrated_indices])[::-1][:n]]\n",
    "\n",
    "    # Map back to book IDs\n",
    "    item_map_rev = {i: b for b, i in item_map.items()}\n",
    "    recommended_books = [(item_map_rev[i], preds[i]) for i in recommended_indices]\n",
    "    return recommended_books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b6207564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display images in a DataFrame\n",
    "def display_images(df, image_column):\n",
    "    # Create an HTML representation of the DataFrame with images\n",
    "    html = df.to_html(escape=False, formatters={\n",
    "        image_column: lambda url: f'<img src=\"{url}\" width=\"100\">'\n",
    "    })\n",
    "    display(HTML(html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "db24bfe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book_id</th>\n",
       "      <th>predicted_rating</th>\n",
       "      <th>title</th>\n",
       "      <th>image_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1052</td>\n",
       "      <td>0.3383</td>\n",
       "      <td>Handle with Care</td>\n",
       "      <td><img src=\"https://images.gr-assets.com/books/1316440644m/3720975.jpg\" width=\"100\"></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1101</td>\n",
       "      <td>0.3370</td>\n",
       "      <td>Change of Heart</td>\n",
       "      <td><img src=\"https://s.gr-assets.com/assets/nophoto/book/111x148-bcc042a9c91a29c1d680899eff700a03.png\" width=\"100\"></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>552</td>\n",
       "      <td>0.3208</td>\n",
       "      <td>The Rescue</td>\n",
       "      <td><img src=\"https://s.gr-assets.com/assets/nophoto/book/111x148-bcc042a9c91a29c1d680899eff700a03.png\" width=\"100\"></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1107</td>\n",
       "      <td>0.3194</td>\n",
       "      <td>Little Earthquakes</td>\n",
       "      <td><img src=\"https://s.gr-assets.com/assets/nophoto/book/111x148-bcc042a9c91a29c1d680899eff700a03.png\" width=\"100\"></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>952</td>\n",
       "      <td>0.3181</td>\n",
       "      <td>Shopaholic & Baby (Shopaholic, #5)</td>\n",
       "      <td><img src=\"https://s.gr-assets.com/assets/nophoto/book/111x148-bcc042a9c91a29c1d680899eff700a03.png\" width=\"100\"></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rcmd = recommend_books(123, R_train, predicted_ratings_matrix)\n",
    "rcmd_df = pd.DataFrame(rcmd, columns=[\"book_id\", \"predicted_rating\"])\n",
    "rcmd_df = rcmd_df.merge(df_books[['book_id', 'title', 'image_url']], on='book_id', how='left')\n",
    "rcmd_df['predicted_rating'] = rcmd_df['predicted_rating'].round(4)\n",
    "display_images(rcmd_df, 'image_url')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "97d4e4d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>book_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>user_idx</th>\n",
       "      <th>item_idx</th>\n",
       "      <th>title</th>\n",
       "      <th>image_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>123</td>\n",
       "      <td>26</td>\n",
       "      <td>5</td>\n",
       "      <td>18464</td>\n",
       "      <td>24</td>\n",
       "      <td>The Da Vinci Code (Robert Langdon, #2)</td>\n",
       "      <td><img src=\"https://images.gr-assets.com/books/1303252999m/968.jpg\" width=\"100\"></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>123</td>\n",
       "      <td>2302</td>\n",
       "      <td>5</td>\n",
       "      <td>18464</td>\n",
       "      <td>2193</td>\n",
       "      <td>Scarlett</td>\n",
       "      <td><img src=\"https://s.gr-assets.com/assets/nophoto/book/111x148-bcc042a9c91a29c1d680899eff700a03.png\" width=\"100\"></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>123</td>\n",
       "      <td>1481</td>\n",
       "      <td>5</td>\n",
       "      <td>18464</td>\n",
       "      <td>1418</td>\n",
       "      <td>Her Fearful Symmetry</td>\n",
       "      <td><img src=\"https://images.gr-assets.com/books/1327939087m/6202342.jpg\" width=\"100\"></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>123</td>\n",
       "      <td>1482</td>\n",
       "      <td>5</td>\n",
       "      <td>18464</td>\n",
       "      <td>1419</td>\n",
       "      <td>The Boston Girl</td>\n",
       "      <td><img src=\"https://images.gr-assets.com/books/1418103945m/22450859.jpg\" width=\"100\"></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>123</td>\n",
       "      <td>1644</td>\n",
       "      <td>5</td>\n",
       "      <td>18464</td>\n",
       "      <td>1574</td>\n",
       "      <td>Peace Like a River</td>\n",
       "      <td><img src=\"https://s.gr-assets.com/assets/nophoto/book/111x148-bcc042a9c91a29c1d680899eff700a03.png\" width=\"100\"></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "user_id = 123\n",
    "\n",
    "# Filter ratings for user 123\n",
    "user_ratings = df_ratings[df_ratings['user_id'] == user_id]\n",
    "\n",
    "# Sort by rating in descending order and get the top 5\n",
    "top_5_actual_ratings = user_ratings.sort_values(by='rating', ascending=False).head(5)\n",
    "\n",
    "# Merge with book details to display titles\n",
    "top_5_actual_ratings = top_5_actual_ratings.merge(df_books[['book_id', 'title', 'image_url']], on='book_id', how='left')\n",
    "\n",
    "# Display the result\n",
    "display_images(top_5_actual_ratings, 'image_url')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fa1d13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed57e86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ea45a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = Reader(rating_scale=(1, 5))\n",
    "data = Dataset.load_from_df(df_ratings[['user_id', 'book_id', 'rating']], reader)\n",
    "trainset, testset = train_test_split(data, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d318e516",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<surprise.prediction_algorithms.matrix_factorization.SVD at 0x18df6ab17f0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from surprise import SVD\n",
    "from surprise import accuracy\n",
    "\n",
    "model = SVD(n_factors=20)  # like latent factor count\n",
    "model.fit(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "344dc8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.test(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "feed3f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def get_top_n_df(predictions, n=5, user_id=None):\n",
    "    top_n = defaultdict(list)\n",
    "    \n",
    "    # Group predictions by user\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        top_n[uid].append((iid, est, true_r))\n",
    "    \n",
    "    # Sort and retain top-n\n",
    "    for uid, user_ratings in top_n.items():\n",
    "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_n[uid] = user_ratings[:n]\n",
    "    \n",
    "    # Filter if user_id is provided\n",
    "    if user_id is not None:\n",
    "        top_n = {user_id: top_n.get(user_id, [])}\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    rows = []\n",
    "    for uid, user_ratings in top_n.items():\n",
    "        for iid, est_rating, true_rating in user_ratings:\n",
    "            rows.append((iid, est_rating, true_rating))\n",
    "    \n",
    "    df_top_n = pd.DataFrame(rows, columns=['book_id', 'predicted_rating', 'actual_rating'])\n",
    "    return df_top_n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "93a3897b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 recommended books for user 2056 :\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book_id</th>\n",
       "      <th>predicted_rating</th>\n",
       "      <th>actual_rating</th>\n",
       "      <th>title</th>\n",
       "      <th>image_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1627</td>\n",
       "      <td>4.463002</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Twelve Sharp (Stephanie Plum, #12)</td>\n",
       "      <td><img src=\"https://images.gr-assets.com/books/1316727699m/3507.jpg\" width=\"100\"></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1606</td>\n",
       "      <td>4.448722</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Ten Big Ones (Stephanie Plum, #10)</td>\n",
       "      <td><img src=\"https://images.gr-assets.com/books/1171076278m/86663.jpg\" width=\"100\"></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>743</td>\n",
       "      <td>4.271887</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Lamb: The Gospel According to Biff, Christ's Childhood Pal</td>\n",
       "      <td><img src=\"https://images.gr-assets.com/books/1331419009m/28881.jpg\" width=\"100\"></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2138</td>\n",
       "      <td>4.239610</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Sizzling Sixteen (Stephanie Plum, #16)</td>\n",
       "      <td><img src=\"https://s.gr-assets.com/assets/nophoto/book/111x148-bcc042a9c91a29c1d680899eff700a03.png\" width=\"100\"></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>207</td>\n",
       "      <td>4.166588</td>\n",
       "      <td>5.0</td>\n",
       "      <td>One for the Money (Stephanie Plum, #1)</td>\n",
       "      <td><img src=\"https://images.gr-assets.com/books/1316730230m/6853.jpg\" width=\"100\"></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "uid=2056\n",
    "df_all = get_top_n_df(predictions, n=5)\n",
    "df_user = get_top_n_df(predictions, n=5, user_id=uid)\n",
    "df_user = df_user.merge(df_books[['book_id', 'title', 'image_url']], on='book_id', how='left')\n",
    "\n",
    "print(\"Top 5 recommended books for user\", uid,\":\")\n",
    "display_images(df_user, 'image_url')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "83c04a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_at_k(predictions, k=5, threshold=4.0):\n",
    "    user_est_true = defaultdict(list)\n",
    "\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        user_est_true[uid].append((iid, est, true_r))\n",
    "\n",
    "    precisions, recalls, f1s = [], [], []\n",
    "    average_precisions, reciprocal_ranks = [], []\n",
    "\n",
    "    for uid, user_ratings in user_est_true.items():\n",
    "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_k = user_ratings[:k]\n",
    "\n",
    "        rel_items = [1 if true_r >= threshold else 0 for (_, _, true_r) in user_ratings]\n",
    "        top_k_rel = [1 if true_r >= threshold else 0 for (_, _, true_r) in top_k]\n",
    "\n",
    "        # Precision, Recall\n",
    "        n_rel = sum(rel_items)\n",
    "        n_rel_k = sum(top_k_rel)\n",
    "\n",
    "        precision = n_rel_k / k if k else 0\n",
    "        recall = n_rel_k / n_rel if n_rel else 0\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "\n",
    "        # F1@K\n",
    "        if precision + recall > 0:\n",
    "            f1 = 2 * (precision * recall) / (precision + recall)\n",
    "        else:\n",
    "            f1 = 0\n",
    "        f1s.append(f1)\n",
    "\n",
    "        # MAP@K\n",
    "        num_hits, sum_prec = 0, 0.0\n",
    "        for i, rel in enumerate(top_k_rel, 1):\n",
    "            if rel:\n",
    "                num_hits += 1\n",
    "                sum_prec += num_hits / i\n",
    "        average_precisions.append(sum_prec / min(n_rel, k) if n_rel else 0)\n",
    "\n",
    "        # MRR@K\n",
    "        rr = 0\n",
    "        for i, rel in enumerate(top_k_rel, 1):\n",
    "            if rel:\n",
    "                rr = 1 / i\n",
    "                break\n",
    "        reciprocal_ranks.append(rr)\n",
    "        \n",
    "    precision = np.mean(precisions)\n",
    "    recall = np.mean(recalls)\n",
    "    f1 = np.mean(f1s)\n",
    "    map = np.mean(average_precisions)\n",
    "    mrr = np.mean(reciprocal_ranks)\n",
    "\n",
    "    return precision, recall, f1, map, mrr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8fa13558",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_ranks(preds, threshold=4.0):\n",
    "    user_est_true = defaultdict(list)\n",
    "    for uid, iid, true_r, est, _ in preds:\n",
    "        user_est_true[uid].append((est, true_r))\n",
    "    \n",
    "    APs = []\n",
    "    RRs = []\n",
    "    for uid, user_ratings in user_est_true.items():\n",
    "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "        num_relevant = 0\n",
    "        precision_sum = 0\n",
    "        rr = 0\n",
    "        for i, (est, true_r) in enumerate(user_ratings):\n",
    "            relevant = true_r >= threshold\n",
    "            if relevant:\n",
    "                num_relevant += 1\n",
    "                precision_sum += num_relevant / (i + 1)\n",
    "                if rr == 0:\n",
    "                    rr = 1 / (i + 1)\n",
    "        if num_relevant > 0:\n",
    "            APs.append(precision_sum / num_relevant)\n",
    "            RRs.append(rr)\n",
    "    \n",
    "    map_score = np.mean(APs)\n",
    "    mrr_score = np.mean(RRs)\n",
    "    return map_score, mrr_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d86b6082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.8327\n",
      "MAE: 0.6461\n",
      "Precision@5: 0.8246\n",
      "Recall@5: 0.3229\n",
      "F1 Score: 0.4414\n",
      "MAP: 0.7769\n",
      "MRR: 0.9283\n"
     ]
    }
   ],
   "source": [
    "rmse = accuracy.rmse(predictions, verbose=False)\n",
    "mae = accuracy.mae(predictions, verbose=False)\n",
    "\n",
    "threshold = 4.0\n",
    "y_true = [int(true_r >= threshold) for (_, _, true_r, _, _) in predictions]\n",
    "y_pred = [int(est >= threshold) for (_, _, _, est, _) in predictions]\n",
    "\n",
    "precision, recall, f1, map_score, mrr_score = metrics_at_k(predictions, k=5, threshold=threshold)\n",
    "\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "\n",
    "print(f\"Precision@5: {precision:.4f}\")\n",
    "print(f\"Recall@5: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"MAP: {map_score:.4f}\")\n",
    "print(f\"MRR: {mrr_score:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
